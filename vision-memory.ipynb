{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9127027,"sourceType":"datasetVersion","datasetId":5495297}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Zero-Shot Image Matching: Playing Memory\n\nRemember the classic memory game? The one where you flip over cards to find matching pairs? Well, it's back with a twist! This time, we’re letting AI do the hard work for us.\n\nMy daughter and I used to play this game together when she was younger. This summer, we decided to bring back the fun and let AI join in on the action. So, we rolled up our sleeves and built an artificial vision algorithm to play the game for us. Spoiler alert: it was awesome!\n\nNow, let me walk you through how we programmed a computer to outmatch us at finding Frozen cards—faster than you can say, \"Let it go!\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-07T11:30:30.251755Z","iopub.execute_input":"2024-08-07T11:30:30.252201Z","iopub.status.idle":"2024-08-07T11:30:31.966255Z","shell.execute_reply.started":"2024-08-07T11:30:30.252166Z","shell.execute_reply":"2024-08-07T11:30:31.964718Z"}}},{"cell_type":"markdown","source":"## Recovering the Data to Test\nAlright, let’s get down to business! To put our AI to the test, we need a set of images featuring our beloved Frozen characters. We’ve prepared a small collection of these images specifically for this experiment.\n\nHere’s the plan: we’ll load these images into our notebook and get them ready for our AI to work its magic. Get ready, Elsa and Anna are about to meet their match!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-07T11:30:30.251755Z","iopub.execute_input":"2024-08-07T11:30:30.252201Z","iopub.status.idle":"2024-08-07T11:30:31.966255Z","shell.execute_reply.started":"2024-08-07T11:30:30.252166Z","shell.execute_reply":"2024-08-07T11:30:31.964718Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom fastprogress.fastprogress import master_bar, progress_bar\nimport math\nimport heapq\n\n# Define the input directory\ninput_dir = Path('/kaggle/input')\n\n# List of all files in the directory\nfiles = sorted([file_path for file_path in input_dir.rglob('*') if file_path.is_file()])\n\nfor i, fn in enumerate(files):\n    print(f\"{i:>4}: {fn}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read all the images an store in the images list\nimages = [cv2.imread(image_path) for image_path in progress_bar(files)]\nprint(f\"{len(images)} images loaded.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility functions to display images\n\ndef show_image(image, ax=None, title=None):\n    \"\"\"\n    Display a single image using matplotlib.\n\n    Parameters:\n    - image: The image to display, expected in BGR format.\n    - ax: Optional; matplotlib axes object to use for plotting.\n    - title: Optional; title for the image plot.\n    \"\"\"\n    # Convert BGR image to RGB for correct color display\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if ax is None:\n        # Create a new plot if no axes object is provided\n        fig, ax = plt.subplots(1, 1, figsize=(16, 16))  # Size of the plot window\n    ax.imshow(image_rgb)\n    if title:\n        ax.set_title(title)  # Set the title if provided\n    ax.axis('off')  # Hide axes for a cleaner look\n\ndef show_images(images, columns=4, titles=None, size_cols=15, size_per_row=3):\n    \"\"\"\n    Display multiple images in a grid using matplotlib.\n\n    Parameters:\n    - images: List of images to display, each expected in BGR format.\n    - columns: Number of columns in the grid.\n    - titles: Optional; list of titles for each image.\n    \"\"\"\n    if titles is None:\n        titles = [None] * len(images)  # Default titles to None if not provided\n    rows = (len(images) + columns - 1) // columns  # Calculate the number of rows needed\n    fig, axes = plt.subplots(rows, columns, figsize=(size_cols, rows * size_per_row))\n\n    # Flatten the axes array for easy iteration\n    axes = axes.flatten()\n\n    # Display each image\n    for ax, image, title in zip(axes, images, titles):\n        show_image(image, ax=ax, title=title)\n\n    # Hide any remaining empty subplots\n    for ax in axes[len(images):]:\n        ax.axis('off')\n\n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n# Example usage:\n# image_path = files[3]\n# image = cv2.imread(image_path)\n# show_image(image)\n\n# Display a grid of images with titles\nshow_images(images, titles=[fn.name for fn in files])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selected Image\nHere’s the image we selected for our experiment. It’s got all the pieces neatly arranged, no pesky lighting issues, and no overlapping cards. A perfect zenithal (top-down) view to make our AI's job a breeze!\n\nLet’s take a look at our beautifully organized Frozen card collection:","metadata":{}},{"cell_type":"code","source":"fn_image_to_test = files[1]  # TODO revisar\nimage_to_test = cv2.imread(fn_image_to_test)\nshow_image(image_to_test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\nFirst things first, let’s get our image ready for some serious AI analysis. We’ll start by performing some preprocessing steps. The goal here is to make it easier for our algorithm to identify and segment the different zones of the image that contain the card pieces.\n\nHere’s the plan:\n\n* Convert to Grayscale: Simplify the image by removing color information.\n* Apply Blur: Smooth out the image to reduce noise and irrelevant details.\n* Binary Thresholding: Create a binary image to clearly distinguish between the card pieces and the background.\n\n\nWith these steps, our image will be prepped and primed for segmentation. Let’s dive in!","metadata":{}},{"cell_type":"code","source":"def preprocess_image(image):\n    \"\"\"\n    Preprocess the image by converting it to grayscale, applying median blur, and thresholding.\n\n    Parameters:\n    - image: The input image in BGR format.\n\n    Returns:\n    - gray: Grayscale version of the input image.\n    - blur: Blurred version of the grayscale image.\n    - thresh: Binary thresholded version of the blurred image.\n    \"\"\"\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # Apply median blur to the grayscale image to reduce noise\n    blur = cv2.medianBlur(gray, 25)\n    \n    # Apply Otsu's thresholding to the blurred image to get a binary image\n    flag, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    return gray, blur, thresh\n\ngray, blur, thresh = preprocess_image(image_to_test)\n\nshow_images((image_to_test, gray, blur, thresh), titles=[\"Original\", \"Grayscale\", \"Blurred\", \"Thresholded\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding Contours\nNow that we have our segmented image, it’s time to find the contours. This is where the magic of old-school computer vision algorithms comes into play.\n\nWe’ll use ```cv2.findContours``` to follow the borders of the objects detected in our binary image. In this simplified black-and-white world, the border is just where black turns to white. By connecting neighboring pixels, we can construct polygons around our card pieces.\n\nHere’s how we refine our approach:\n\n* Filter by Area: Discard polygons that are too small, assuming all card pieces should be of similar size.\n* Simplify Polygons: Use cv2.approxPolyDP to approximate each contour with a simpler polygon, ideally with four sides (rectangles or trapezoids).\n\nCross your fingers and hope for good lighting and minimal overlaps! If all goes well, we’ll have a nice set of polygons representing our card pieces, ready for further analysis.\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:05:06.708737Z","iopub.execute_input":"2024-08-07T19:05:06.709176Z","iopub.status.idle":"2024-08-07T19:05:06.716774Z","shell.execute_reply.started":"2024-08-07T19:05:06.709142Z","shell.execute_reply":"2024-08-07T19:05:06.714859Z"}}},{"cell_type":"code","source":"def apply_countours(image, contours, color=(0, 255, 0), size=10):\n    \"\"\"\n    Draw contours on the image.\n\n    Parameters:\n    - image: The input image on which contours will be drawn.\n    - contours: List of contours to be drawn.\n    - color: Optional; color of the contours (default is red).\n    - size: Optional; thickness of the contour lines (default is 10).\n\n    Returns:\n    - image: Image with contours drawn on it.\n    \"\"\"\n    # Make a copy of the image to avoid altering the original\n    image = image.copy()\n    \n    # Draw contours on the image\n    cv2.drawContours(image, contours, -1, color, size)\n    \n    return image\n\n\ndef find_cards(thresh):\n    \"\"\"\n    Find and filter contours in the thresholded image to identify potential cards.\n\n    Parameters:\n    - thresh: Binary thresholded image.\n\n    Returns:\n    - cards: List of approximated polygons representing the detected cards.\n    \"\"\"\n    # Find contours in the thresholded image\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Calculate the area of each contour\n    areas = [cv2.contourArea(contour) for contour in contours]\n    \n    # Define the area bounds based on the median area of significant contours (removing too small polygons)\n    lower_bound = np.median([a for a in areas if a > 100]) * 0.50  # Minimal area filter\n    upper_bound = np.median([a for a in areas if a > 100]) * 1.50  # Upper bound filter\n\n    # Filter contours based on the defined area bounds\n    interesting_contours = [contour for contour in contours if lower_bound <= cv2.contourArea(contour) <= upper_bound]\n    discarded_contours = [contour for contour in contours if not(lower_bound <= cv2.contourArea(contour) <= upper_bound)]\n    \n    # Approximate polygons for the filtered contours\n    cards = [cv2.approxPolyDP(contour, 0.02 * cv2.arcLength(contour, True), True) for contour in interesting_contours]\n    \n    return cards\nshow_image(apply_countours(image_to_test, cards:=find_cards(thresh)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Labeling Cards\nNow that we’ve detected all the cards, it’s time to label them. We’ll draw a label on each card to keep track of our findings. This step is crucial for verifying our AI’s accuracy and ensuring everything is correctly identified.\n\nHere’s what we’ll do:\n\n* Draw Contours: Highlight each detected card by drawing its contour.\n* Add Labels: Assign a unique label to each card, making it easy to reference and verify.\n\nLet’s visualize our labeled cards and see how our AI did in identifying the pieces!","metadata":{"execution":{"iopub.status.busy":"2024-08-07T18:15:41.957825Z","iopub.execute_input":"2024-08-07T18:15:41.958176Z","iopub.status.idle":"2024-08-07T18:15:48.174165Z","shell.execute_reply.started":"2024-08-07T18:15:41.958144Z","shell.execute_reply":"2024-08-07T18:15:48.172876Z"}}},{"cell_type":"code","source":"def image_labeled_cards(image, cards, labeler=None, fontScale=5, thickness=30, contour_thickness=40):\n    \"\"\"\n    Draw contours and labels on detected card regions in an image.\n\n    Parameters:\n    - image: The input image on which labels and contours will be drawn.\n    - cards: List of card contours to be labeled.\n    - labeler: Optional; function to generate labels for each card (default is None).\n    - fontScale: Optional; scale of the font used for labels (default is 5).\n    - thickness: Optional; thickness of the text (default is 30).\n    - contour_thickness: Optional; thickness of the contour lines (default is 40).\n\n    Returns:\n    - image_labeled: Image with labeled contours.\n    \"\"\"\n    # Make a copy of the image to avoid altering the original\n    image_labeled = image.copy()\n    \n    # Use a colormap for contour colors\n    colormap = plt.get_cmap(\"tab20\")\n\n    for i, card in enumerate(cards):\n        # Calculate the bounding rectangle of the contour\n        x, y, w, h = cv2.boundingRect(card)\n        cX = x + w // 2\n        cY = y + h // 2\n\n        # Determine the color and label for the current card\n        if labeler:\n            color_index, label = labeler(i)\n        else:\n            color_index, label = i, None\n        \n        # Convert colormap color to BGR format\n        color = tuple(int(c * 256) for c in colormap.colors[color_index % len(colormap.colors)])\n        \n        # Draw the contour on the image\n        cv2.drawContours(image_labeled, [card], 0, color, contour_thickness)\n\n        if label is not None:\n            # Get the size of the text to be drawn\n            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, fontScale, thickness)[0]\n\n            # Calculate the text position to be centered\n            textX = cX - text_size[0] // 2\n            textY = cY + text_size[1] // 2\n\n            # Put the text in the center of the bounding rectangle\n            cv2.putText(image_labeled, label, (textX, textY), cv2.FONT_HERSHEY_SIMPLEX, fontScale, color, thickness, cv2.LINE_AA)\n\n    return image_labeled\n\n# Apply the labeled contours to the test image and display the result\nshow_image(image_labeled_cards(image_to_test, cards, labeler=lambda x: (x, f\"{x}\")))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Warping\nNow for the fun part—warping! We need to zoom in on each card piece and convert it to a uniform size. This helps standardize the pieces for further analysis. To achieve this, we use ```cv2.getPerspectiveTransform``` to correct any perspective distortions.\n\nHere’s the game plan:\n\n* Zoom In: Focus on each detected card piece.\n* Standardize Size: Convert each piece to the same size for consistency.\n* Correct Perspective: Use cv2.getPerspectiveTransform to adjust for any perspective effects, ensuring each card is viewed head-on.\n\nThis process will help us create a clean, uniform dataset of card images, ready for AI to work its matching magic!","metadata":{}},{"cell_type":"code","source":"WARP_SIZE = 200\n\ndef obtain_card_warped(image, cards):\n    \"\"\"\n    Warp the detected card regions in the image to a fixed size.\n\n    Parameters:\n    - image: The input image containing the detected card regions.\n    - cards: List of contours representing the card regions.\n\n    Returns:\n    - warped_cards: List of images of the warped card regions.\n    \"\"\"\n    numcards = len(cards)\n    warped_cards = [None] * numcards\n\n    for i in range(numcards):\n        approx = cards[i]\n        # Flatten the contour points to a numpy array of float32\n        approx = np.array([item for sublist in approx for item in sublist], np.float32)\n        \n        # Define the destination points for the perspective transform\n        h = np.array([[0, 0], [0, WARP_SIZE], [WARP_SIZE, WARP_SIZE], [WARP_SIZE, 0]], np.float32)\n        \n        # Get the perspective transform matrix\n        transform = cv2.getPerspectiveTransform(approx, h)\n        \n        # Apply the perspective transform to get the warped image\n        warped_cards[i] = cv2.warpPerspective(image, transform, (WARP_SIZE, WARP_SIZE))\n\n    return warped_cards\n\n# Warp the card regions in the test image\nwarped_cards = obtain_card_warped(image_to_test, cards)\n\n# Display the warped card images\nshow_images(warped_cards, columns=16, titles=[f\"{i}\" for i in range(len(warped_cards))], size_per_row=2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating Similarity\nWith all our card pieces now standardized in size, it’s time to detect which images are similar to each other. We’ll use a simple yet effective approach to compare the images.\n\nHere’s our plan:\n\n* Image Subtraction: Subtract one image from another to highlight differences.\n* Mean Square Root: Calculate the Mean Square Root of these differences across the entire image. This gives us a quantitative measure of similarity.\n\n\nWe can’t assume the images are correctly **oriented**. But here’s the good news—each card piece has four sides, so we only need to check four possible orientations.\nBy following this method, we’ll determine which cards are pairs based on their similarity scores. Let’s dive into the calculations!","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:25:43.887215Z","iopub.execute_input":"2024-08-07T19:25:43.887576Z","iopub.status.idle":"2024-08-07T19:25:43.893034Z","shell.execute_reply.started":"2024-08-07T19:25:43.887545Z","shell.execute_reply":"2024-08-07T19:25:43.891026Z"}}},{"cell_type":"code","source":"def image_distance(img1, img2):\n    \"\"\"\n    Calculate the mean squared error (MSE) between two images.\n\n    Parameters:\n    - img1: First image in numpy array format.\n    - img2: Second image in numpy array format.\n\n    Returns:\n    - mse: Mean squared error between the two images.\n    \"\"\"\n    img1 = img1.astype(np.float32)\n    img2 = img2.astype(np.float32)\n    mse = np.mean((img1 - img2) ** 2)\n    return mse\n\ndef best_image_distance(img1, img2):\n    \"\"\"\n    Find the best rotation of the second image that minimizes the distance to the first image.\n\n    Parameters:\n    - img1: First image in numpy array format.\n    - img2: Second image in numpy array format.\n\n    Returns:\n    - min_distance: Minimum MSE between the first image and the best rotation of the second image.\n    - best_rotation: The rotation of the second image that gives the minimum MSE.\n    \"\"\"\n    # Generate all 90-degree rotations of the second image\n    rotations = [img2]\n    for _ in range(3):\n        rotations.append(np.rot90(rotations[-1]))\n\n    # Calculate the MSE for each rotation\n    distances = [image_distance(img1, rotation) for rotation in rotations]\n\n    # Find the rotation with the minimum MSE\n    min_distance = min(distances)\n    best_rotation = rotations[distances.index(min_distance)]\n\n    return min_distance, best_rotation\n\n\ndef test_pair(a,b):\n    distance, best_rotation = best_image_distance(warped_cards[a], warped_cards[b])\n    diff = np.abs(warped_cards[a].astype(np.float32)-best_rotation.astype(np.float32)).astype(np.uint8)\n\n    show_images([warped_cards[a], warped_cards[b], best_rotation, diff],\n               titles=[ \"Target\", \"Candidate\", f\"Best rotation : {distance:.2f}\", \"Diff\"] )\n\ntest_pair(0, 1) # shows the test againt two images, and the best rotation ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating the Similarity for All Cards\nNow, it's time to put our similarity method to the test and compare all the cards against each other. By doing this, we can identify the best matches and locate the pairs.\n\nHere’s how we’ll do it:\n\n* Pairwise Comparison: Compare each card with every other card using our image subtraction and Mean Square Root method.\n* Find Best Matches: Determine the best match for each card based on the lowest distance score.\n* Identify Pairs: Use these best matches to locate the pairs of cards.\nBy systematically comparing all the cards, we’ll be able to uncover the pairs efficiently. Let’s get matching!","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:25:43.913904Z","iopub.execute_input":"2024-08-07T19:25:43.914355Z","iopub.status.idle":"2024-08-07T19:25:43.927729Z","shell.execute_reply.started":"2024-08-07T19:25:43.914314Z","shell.execute_reply":"2024-08-07T19:25:43.926525Z"}}},{"cell_type":"code","source":"numcards = len(warped_cards)\n\n\n\n# Define the number of elements to plot\nELEMENTS_TO_PLOT = 5\nELEMENTS_TO_PLOT = min(ELEMENTS_TO_PLOT, numcards)\n\n# Define the number of best elements to study\nBEST_ELEMENTS_TO_STUDY = 5\nBEST_ELEMENTS_TO_STUDY = min(BEST_ELEMENTS_TO_STUDY, numcards - 1)\npairs = {}\nfig, axs = plt.subplots(ELEMENTS_TO_PLOT, BEST_ELEMENTS_TO_STUDY + 1, figsize=(16, 3 * ELEMENTS_TO_PLOT))\nmb = master_bar(warped_cards)\n\nfor i, card_selected in enumerate(mb):\n    best_options = []\n    for j in progress_bar(range(numcards), total=numcards, parent=mb):\n        if i != j:\n            distance, best_rotation = best_image_distance(card_selected, warped_cards[j])\n            best_options.append((distance, j, best_rotation))\n            best_options.sort(key=lambda x: x[0])\n            best_options = best_options[:BEST_ELEMENTS_TO_STUDY]\n\n    best_pair = best_options[0][1]\n    pairs[i] = best_pair\n    if i < ELEMENTS_TO_PLOT:\n        ax = axs[i]\n        show_image(card_selected, ax=ax[0], title=f\"#{i} => #{best_pair}\")\n        for k, (ratio, j, best_rotation) in enumerate(best_options):\n            show_image(best_rotation, ax=ax[k + 1], title=f\"#{j} {ratio:.2f}\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pair id definition\npair_id = {}\nid = 0\nfor k,v in pairs.items():\n    if k not in pair_id:\n        pair_id[k] = id\n        assert v not in pair_id, f\"{k,v} => {pair_id[v]}\"   # this should work if all the mathc has a mutual match\n        pair_id[v] = id\n        id+=1\nprint(pair_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\nDrumroll, please! It’s time to reveal the results of our AI-powered memory game experiment. After processing and comparing all the cards, here’s what we found:\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:52:50.498125Z","iopub.execute_input":"2024-08-07T19:52:50.498560Z","iopub.status.idle":"2024-08-07T19:52:56.459031Z","shell.execute_reply.started":"2024-08-07T19:52:50.498523Z","shell.execute_reply":"2024-08-07T19:52:56.457806Z"}}},{"cell_type":"code","source":"show_image(image_labeled_cards(image_to_test, cards, thickness=15, labeler=lambda x: (pair_id[x], f\"{chr(pair_id[x]+ord('A'))}\")))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nThese results highlight the potential of using AI to tackle classic games like memory. Who knew that Elsa, Anna, and a bit of computer vision could make such a great team?\n\nThis project was a delightful summer endeavor shared between a father and daughter, combining play and learning. We had a blast experimenting with various tools and algorithms, discovering how they can solve puzzles and make games more fun. \n\nOur experiment not only demonstrated the power of AI in gaming but also created wonderful memories. Who knows what we’ll tackle next? The possibilities are endless when you mix family fun with a dash of AI magic!\n\nMaybe not many people enjoy making the computer solve puzzles. But I'm lucky 🍀, I'm not alone.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}